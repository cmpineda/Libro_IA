{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9ydyrdHpjBY"
   },
   "source": [
    "## Ejemplo Modelo Seq2Seq Sin atención"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "3Co6KBHcz0ck"
   },
   "outputs": [],
   "source": [
    "def quitarchar(texto):\n",
    "  texto = texto.replace(\"¡\", \"\").replace(\"¿\", \"\")\n",
    "  return texto.strip()\n",
    "\n",
    "with open('spa.txt','r',encoding='utf8') as f:\n",
    "  datos=[(quitarchar(x.split('\\t')[0]),quitarchar(x.split('\\t')[1])) for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bkL9D99U0TJP",
    "outputId": "3b39301d-af87-4e4b-d46b-64281a786303"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Go.', 'Ve.'),\n",
       " ('Go.', 'Vete.'),\n",
       " ('Go.', 'Vaya.'),\n",
       " ('Go.', 'Váyase.'),\n",
       " ('Hi.', 'Hola.'),\n",
       " ('Run!', 'Corre!'),\n",
       " ('Run!', 'Corran!'),\n",
       " ('Run!', 'Huye!'),\n",
       " ('Run!', 'Corra!'),\n",
       " ('Run!', 'Corred!')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "4q9SsTpk0hVv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.shuffle(datos)\n",
    "palabras_ingles, palabras_español = zip(*datos)\n",
    "long_vocab_ingles = len(palabras_ingles)\n",
    "long_vocab_español = len(palabras_español)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52RZivBLzGJz",
    "outputId": "faaf57f9-7478-4a7f-f0a1-1946ab10dfab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud Vocab Español: 141543\n",
      "Logitud Vocab Inglés: 141543\n"
     ]
    }
   ],
   "source": [
    "print(f'Longitud Vocab Español: {long_vocab_ingles}')\n",
    "print(f'Logitud Vocab Inglés: {long_vocab_español}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7jtW9ZvS0l0e",
    "outputId": "fa188cef-0c70-4724-bb27-27aac675c1c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm single again. => Estoy solo otra vez.\n",
      "Don't leave your belongings unattended at the beach. => No dejes tus pertenencias desatendidas en el playa.\n",
      "You're joking, aren't you? => Estás bromeando, verdad?\n",
      "Why take a chance? => Para qué correr el riesgo?\n",
      "There are many more students in the classroom today than yesterday. => Hoy hay muchos más estudiantes en la sala que ayer.\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(palabras_ingles[i], \"=>\", palabras_español[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hvGqjInFtp_P",
    "outputId": "fbb6f082-322d-433e-b7f8-f9e5971aedce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Longitud Frases en Inglés: 70\n",
      "Max Longitud Frases en Español: 68\n",
      "Logitud de la secuencia: 70\n"
     ]
    }
   ],
   "source": [
    "long_ing_max = max(len(linea.split()) for linea in palabras_ingles)\n",
    "long_esp_max = max(len(linea.split()) for linea in palabras_español)\n",
    "long_secuencia = max(long_ing_max, long_esp_max)\n",
    "\n",
    "print(f'Max Longitud Frases en Inglés: {long_ing_max}')\n",
    "print(f'Max Longitud Frases en Español: {long_esp_max}')\n",
    "print(f'Logitud de la secuencia: {long_secuencia}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "7EzLcdBe0yK7"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import TextVectorization\n",
    "\n",
    "tam_voc = 1000\n",
    "long_max = 70\n",
    "capa_vect_ing = tf.keras.layers.TextVectorization(\n",
    "    tam_voc, output_sequence_length=long_max)\n",
    "capa_vect_esp = tf.keras.layers.TextVectorization(\n",
    "    tam_voc, output_sequence_length=long_max)\n",
    "capa_vect_ing.adapt(palabras_ingles)\n",
    "capa_vect_esp.adapt([f\"startseq {s} endseq\" for s in palabras_español])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G4gsr8EZ03h3",
    "outputId": "aeeb89f4-3f39-4a6a-cc26-0112fe9b4502"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inglés:\n",
      "['', '[UNK]', 'i', 'the', 'to', 'you', 'tom', 'a', 'is', 'he']\n",
      "\n",
      "Español:\n",
      "['', '[UNK]', 'startseq', 'endseq', 'de', 'que', 'no', 'tom', 'a', 'la']\n"
     ]
    }
   ],
   "source": [
    "print(\"Inglés:\")\n",
    "print(capa_vect_ing.get_vocabulary()[:10])\n",
    "\n",
    "print(\"\\nEspañol:\")\n",
    "print(capa_vect_esp.get_vocabulary()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "bXYHFdu-08wi"
   },
   "outputs": [],
   "source": [
    "X_train = tf.constant(palabras_ingles[:120_000])\n",
    "X_valid = tf.constant(palabras_ingles[120_000:])\n",
    "X_train_dec = tf.constant([f\"startseq {s}\" for s in palabras_español[:120_000]])\n",
    "X_valid_dec = tf.constant([f\"startseq {s}\" for s in palabras_español[120_000:]])\n",
    "Y_train = capa_vect_esp([f\"{s} endseq\" for s in palabras_español[:120_000]])\n",
    "Y_valid = capa_vect_esp([f\"{s} endseq\" for s in palabras_español[120_000:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "PTbMXAFP1Bg9"
   },
   "outputs": [],
   "source": [
    "ent_cod = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "ent_dec = tf.keras.layers.Input(shape=[], dtype=tf.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "6N8w1MeN1Egw"
   },
   "outputs": [],
   "source": [
    "tam_emb = 128\n",
    "ids_ent_cod = capa_vect_ing(ent_cod)\n",
    "ids_ent_dec = capa_vect_esp(ent_dec)\n",
    "\n",
    "capa_embedding_cod = tf.keras.layers.Embedding(tam_voc, tam_emb,\n",
    "                                                    mask_zero=True)\n",
    "capa_embedding_dec = tf.keras.layers.Embedding(tam_voc, tam_emb,\n",
    "                                                    mask_zero=True)\n",
    "embeddings_cod = capa_embedding_cod(ids_ent_cod)\n",
    "embeddings_dec = capa_embedding_dec(ids_ent_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "w3XUYYGW1FbW"
   },
   "outputs": [],
   "source": [
    "#creamos el codificador\n",
    "codificador = tf.keras.layers.LSTM(512, return_state=True)\n",
    "salidas_cod, *estado_cod = codificador(embeddings_cod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "yGRCh5mM1L27"
   },
   "outputs": [],
   "source": [
    "#creamos el decodificador\n",
    "decodificador = tf.keras.layers.LSTM(512, return_sequences=True)\n",
    "salidas_dec = decodificador(embeddings_dec, initial_state=estado_cod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "GkPwmrH81O0I"
   },
   "outputs": [],
   "source": [
    "capa_salida = tf.keras.layers.Dense(tam_voc, activation=\"softmax\")\n",
    "Y_proba = capa_salida(salidas_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "id": "lQLEAglW1XLW",
    "outputId": "d967c3ac-2876-4d7b-8d50-d97ce9fe17f1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_6             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                 │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ input_layer_7             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                 │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ text_vectorization_6      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)       │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ text_vectorization_7      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)       │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ embedding_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128,000</span> │ text_vectorization_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ not_equal_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ text_vectorization_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ embedding_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128,000</span> │ text_vectorization_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,312,768</span> │ embedding_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)]     │                │ not_equal_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ lstm_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,312,768</span> │ embedding_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
       "│                           │                        │                │ lstm_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],          │\n",
       "│                           │                        │                │ lstm_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]           │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">513,000</span> │ lstm_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
       "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_6             │ (\u001b[38;5;45mNone\u001b[0m)                 │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ input_layer_7             │ (\u001b[38;5;45mNone\u001b[0m)                 │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ text_vectorization_6      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ input_layer_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mTextVectorization\u001b[0m)       │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ text_vectorization_7      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ input_layer_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mTextVectorization\u001b[0m)       │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ embedding_8 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m128,000\u001b[0m │ text_vectorization_6[\u001b[38;5;34m…\u001b[0m │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ not_equal_6 (\u001b[38;5;33mNotEqual\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ text_vectorization_6[\u001b[38;5;34m…\u001b[0m │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ embedding_9 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m128,000\u001b[0m │ text_vectorization_7[\u001b[38;5;34m…\u001b[0m │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)             │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   │      \u001b[38;5;34m1,312,768\u001b[0m │ embedding_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│                           │ \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)]     │                │ not_equal_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ lstm_7 (\u001b[38;5;33mLSTM\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │      \u001b[38;5;34m1,312,768\u001b[0m │ embedding_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
       "│                           │                        │                │ lstm_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],          │\n",
       "│                           │                        │                │ lstm_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]           │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m, \u001b[38;5;34m1000\u001b[0m)       │        \u001b[38;5;34m513,000\u001b[0m │ lstm_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
       "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,394,536</span> (12.95 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,394,536\u001b[0m (12.95 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,394,536</span> (12.95 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,394,536\u001b[0m (12.95 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modelo = tf.keras.Model(inputs=[ent_cod, ent_dec],\n",
    "                       outputs=[Y_proba])\n",
    "modelo.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5H3Ney4m2Fyy",
    "outputId": "a7cec51a-abe5-4635-883f-c4a3d19a5198"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 27ms/step - accuracy: 0.0375 - loss: 3.4009 - val_accuracy: 0.0549 - val_loss: 2.0048\n",
      "Epoch 2/5\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 27ms/step - accuracy: 0.0576 - loss: 1.8417 - val_accuracy: 0.0633 - val_loss: 1.5206\n",
      "Epoch 3/5\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 27ms/step - accuracy: 0.0658 - loss: 1.3808 - val_accuracy: 0.0665 - val_loss: 1.3532\n",
      "Epoch 4/5\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 28ms/step - accuracy: 0.0707 - loss: 1.1399 - val_accuracy: 0.0678 - val_loss: 1.2969\n",
      "Epoch 5/5\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 28ms/step - accuracy: 0.0742 - loss: 0.9726 - val_accuracy: 0.0682 - val_loss: 1.2909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7e600d3beb60>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.fit((X_train, X_train_dec), Y_train, epochs=5,\n",
    "          validation_data=((X_valid, X_valid_dec), Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "1KdiPWn12Liw"
   },
   "outputs": [],
   "source": [
    "def traducir(oracion_ingles):\n",
    "    traduccion = \"\"\n",
    "    for id_palabra in range(long_max):\n",
    "        X = tf.constant([oracion_ingles])  # codificar entrada\n",
    "        X_dec = tf.constant([\"startseq \" + traduccion])  # decodificar entrada\n",
    "        y_proba = modelo.predict((X, X_dec))[0, id_palabra]  # probabilidad del último token\n",
    "        id_palabra_predicha = np.argmax(y_proba)\n",
    "        palabra_predicha = capa_vect_esp.get_vocabulary()[id_palabra_predicha]\n",
    "        if palabra_predicha == \"endseq\":\n",
    "            break\n",
    "        traduccion += \" \" + palabra_predicha\n",
    "    return traduccion.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "7QQqcsjM3UrG",
    "outputId": "fbfb8f10-d49d-41ff-d9b3-eb3a4b895a08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 355ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'me gusta el fútbol'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traducir(\"I like soccer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dG6zE0YM4MX6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJIZoEOLpomw"
   },
   "source": [
    "## Ejemplo Modelo Seq2Seq con atención"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "JzN22Omp8-ut"
   },
   "outputs": [],
   "source": [
    "#Usamos una capa Bidireccional para el codificador\n",
    "codificador = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(256, return_sequences=True, return_state=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VfOFSLK0p2_B",
    "outputId": "878f197a-a8be-4a12-e1bb-3e8ad8c860fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'lambda_16' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Asegúrate de que salidas_cod tenga la forma adecuada\n",
    "salidas_cod_exp = tf.keras.layers.Lambda(\n",
    "    lambda x: tf.expand_dims(x, axis=1),  # Expande para tener una dimensión de secuencia\n",
    "    output_shape=(1, salidas_cod.shape[-1])  # Define la forma de salida\n",
    ")(salidas_cod)\n",
    "\n",
    "salidas_dec_exp = tf.keras.layers.Lambda(\n",
    "    lambda x: tf.expand_dims(x, axis=1),\n",
    "    output_shape=(1, salidas_dec.shape[-1])\n",
    ")(salidas_dec)\n",
    "\n",
    "# Capa de atención\n",
    "capa_atencion = tf.keras.layers.Attention()\n",
    "\n",
    "# Calcula la salida de atención\n",
    "salida_atencion = capa_atencion([salidas_dec_exp, salidas_cod_exp])\n",
    "\n",
    "# Agrega la capa de salida\n",
    "capa_salida = tf.keras.layers.Dense(tam_voc, activation=\"softmax\")\n",
    "Y_proba = capa_salida(salida_atencion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 192
    },
    "id": "f9sXoaopqSna",
    "outputId": "19a1f7d8-8dff-4f8f-8d96-716ba7ae560c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'me gusta jugar al tenis con mis amigos'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traducir(\"I like play soccer with my friends\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incrustación de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding para 'I': [-0.23227006 -0.17839591  0.16077961  0.22499053]\n",
      "Embedding para 'am': [-0.01326084  0.00521127  0.12721364  0.22602785]\n",
      "Embedding para 'strong': [-0.12546264 -0.09407401  0.1838298  -0.03889734]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import Text8Corpus  # Puedes usar tu propio corpus\n",
    "\n",
    "# Corpus de ejemplo\n",
    "oraciones = [\n",
    "    [\"I\", \"am\", \"strong\",'today'],\n",
    "    [\"I\", \"am\", \"sad\",'today'],\n",
    "    [\"She\", \"is\", \"happy\",'always'],\n",
    "    [\"He\", \"likes\", \"reading\",\"books\"],    \n",
    "    [\"I\", \"am\", \"strong\",'friend'],\n",
    "    [\"I\", \"am\", \"strong\",'forever'],\n",
    "]\n",
    "\n",
    "# Entrenar el modelo Word2Vec\n",
    "modelo = Word2Vec(oraciones, vector_size=4, window=2, sg=0, min_count=1, epochs=100)\n",
    "\n",
    "# Obtener los embeddings de las palabras\n",
    "vectores_palabras = modelo.wv\n",
    "\n",
    "# Obtenemos los embeddings para \"I\", \"am\", \"strong\"\n",
    "embedding_i = vectores_palabras[\"I\"]\n",
    "embedding_am = vectores_palabras[\"am\"]\n",
    "embedding_strong = vectores_palabras[\"strong\"]\n",
    "\n",
    "print(\"Embedding para 'I':\", embedding_i) \n",
    "print(\"Embedding para 'am':\", embedding_am)\n",
    "print(\"Embedding para 'strong':\", embedding_strong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.23227006 -0.17839591  0.16077961  0.22499053]\n",
      " [-0.01326084  0.00521127  0.12721364  0.22602785]\n",
      " [-0.12546264 -0.09407401  0.1838298  -0.03889734]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([embedding_i,embedding_am,embedding_strong])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud entre embeddings de I y am: 0.7020716\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(\"Similitud entre embeddings de I y am:\", cosine_similarity(embedding_i.reshape(1, -1), embedding_am.reshape(1, -1))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensión de las incrustaciones y la cantidad de palabras a procesar\n",
    "embedding_dim=4\n",
    "dk=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificación Posicional (Positional Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codificación posicional:\n",
      "[[ 0.          1.          0.          1.        ]\n",
      " [ 0.84147098  0.54030231  0.00999983  0.99995   ]\n",
      " [ 0.90929743 -0.41614684  0.01999867  0.99980001]]\n",
      "Embeddings con codificación posicional:\n",
      "[[-0.23227006  0.8216041   0.16077961  1.2249905 ]\n",
      " [ 0.8282101   0.5455136   0.13721347  1.2259779 ]\n",
      " [ 0.7838348  -0.5102208   0.20382847  0.9609027 ]]\n"
     ]
    }
   ],
   "source": [
    "def codificacion_posicional(seq_len, d_model):\n",
    "    PE = np.zeros((seq_len, d_model))\n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, d_model, 2):\n",
    "            PE[pos, i] = np.sin(pos / (10000 ** (i / d_model)))\n",
    "            if i + 1 < d_model:\n",
    "                PE[pos, i + 1] = np.cos(pos / (10000 ** (i / d_model)))\n",
    "    return PE\n",
    "\n",
    "cod_pos = codificacion_posicional(len(X), embedding_dim)\n",
    "X += cod_pos\n",
    "\n",
    "print(\"Codificación posicional:\")\n",
    "print(cod_pos)\n",
    "print(\"Embeddings con codificación posicional:\")\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-Atención (Self-Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#se crean 3 matrices Q,K y V de 4x4. Las cuales tienen valores que se consiguen con el entrenamiento.\n",
    "W_Q = np.random.rand(embedding_dim, dk)\n",
    "W_K = np.random.rand(embedding_dim, dk)\n",
    "W_V = np.random.rand(embedding_dim, dk)\n",
    "\n",
    "Q = X @ W_Q\n",
    "K = X @ W_K\n",
    "V = X @ W_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.71913102 1.66648306 0.83114777]\n",
      " [1.4334741  1.8154474  0.94406603]\n",
      " [1.07814215 0.57159013 0.0024822 ]] \n",
      "\n",
      "[[1.53919814 0.97260947 1.21558353]\n",
      " [2.26464446 1.14081689 2.03522851]\n",
      " [1.09853186 0.27237031 1.34744961]] \n",
      "\n",
      "[[0.42430665 1.04209421 0.56534287]\n",
      " [0.56583229 1.35927576 0.60996333]\n",
      " [0.29281406 0.9436194  0.3191888 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Q, \"\\n\"), print(K, \"\\n\"), print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculo de las puntuaciones y normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de atención (Z):\n",
      " [[0.17666947 0.77862724 0.0447033 ]\n",
      " [0.10534776 0.87685887 0.01779337]\n",
      " [0.2611982  0.62992073 0.10888107]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "puntajes_atencion = Q @ K.T / math.floor(np.sqrt(dk))\n",
    "\n",
    "# Aplicar Softmax\n",
    "pesos_atención = np.exp(puntajes_atencion) / np.sum(np.exp(puntajes_atencion), axis=1, keepdims=True)\n",
    "print(\"Matriz de atención (Z):\\n\", pesos_atención)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtener la Salida de Atención"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salida de atención (Z):\n",
      " [[0.52862422 1.28465826 0.58908168]\n",
      " [0.54606497 1.31846547 0.60008881]\n",
      " [0.49913953 1.2311714  0.56664871]]\n"
     ]
    }
   ],
   "source": [
    "Z = pesos_atención @ V\n",
    "print(\"Salida de atención (Z):\\n\", Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capa Feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salida de la red:\n",
      " [[10.20958739  6.60023588  6.99729101 10.00462157]\n",
      " [10.35935582  6.70236674  7.10295769 10.16127688]\n",
      " [ 9.95360495  6.42520503  6.8162129   9.73666035]]\n"
     ]
    }
   ],
   "source": [
    "W1 = np.random.rand(dk, 2 * embedding_dim)\n",
    "b1 = np.random.rand(2 * embedding_dim)\n",
    "W2 = np.random.rand(2 * embedding_dim, embedding_dim)\n",
    "b2 = np.random.rand(embedding_dim)\n",
    "\n",
    "def propagacion(x):\n",
    "    return np.maximum(0, x @ W1 + b1) @ W2 + b2\n",
    "\n",
    "Z_salida = propagacion(Z)\n",
    "print(\"Salida de la red:\\n\", Z_salida)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d61GJUrBDrZO"
   },
   "source": [
    "## Ejemplo Modelo con Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "Ef-pnAfGFlNS"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Dense\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "UXB0TNWjOHJo"
   },
   "outputs": [],
   "source": [
    "long_max = 70  # maxima longitud de palabra\n",
    "tam_emb = 128\n",
    "tf.random.set_seed(42)\n",
    "pos_capa_emb = tf.keras.layers.Embedding(long_max, tam_emb)\n",
    "max_long_lote_cod = tf.keras.backend.int_shape(embeddings_cod)[1]\n",
    "\n",
    "ent_codif = embeddings_cod + pos_capa_emb(tf.range(max_long_lote_cod))\n",
    "max_long_lote_dec = tf.keras.backend.int_shape(embeddings_dec)[1]\n",
    "\n",
    "ent_decodif = embeddings_cod + pos_capa_emb(tf.range(max_long_lote_dec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "A4kEFM97BdjA"
   },
   "outputs": [],
   "source": [
    "# Funciones auxiliares para codificación posicional\n",
    "def obtener_angulos(pos, i, d_model):\n",
    "    angulos = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angulos\n",
    "\n",
    "def codificacion_posicional(posicion, d_model):\n",
    "    radianes = obtener_angulos(np.arange(posicion)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n",
    "    radianes[:, 0::2] = np.sin(radianes[:, 0::2])\n",
    "    radianes[:, 1::2] = np.cos(radianes[:, 1::2])\n",
    "    pos_codificacion = radianes[np.newaxis, ...]\n",
    "    return tf.cast(pos_codificacion, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "DGUrP_3JCLXA"
   },
   "outputs": [],
   "source": [
    "#d_model = 128\n",
    "entradas = Input(shape=(None,))\n",
    "etiquetas = Input(shape=(None,))\n",
    "\n",
    "# Embedding y codificación posicional para el codificador\n",
    "cod_pos_cod = codificacion_posicional(long_max, tam_emb)\n",
    "embeddings_cod += cod_pos_cod[:tf.keras.backend.int_shape(embeddings_cod)[1], :]\n",
    "\n",
    "# Embedding y codificación posicional para el decodificador\n",
    "dec_pos_encoding = codificacion_posicional(long_max, tam_emb)\n",
    "embeddings_dec += dec_pos_encoding[:tf.keras.backend.int_shape(embeddings_dec)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "MlAgXm86PEIR"
   },
   "outputs": [],
   "source": [
    "#Codificador\n",
    "N = 2\n",
    "num_cabezas = 8\n",
    "tasa_dropout = 0.1\n",
    "n_unidades = 128\n",
    "\n",
    "# se encapsula la operación en una capa landa que es compatible con un modelo simbólica\n",
    "masc_cod = tf.keras.layers.Lambda(\n",
    "    lambda x: tf.math.not_equal(x, 0)[:, tf.newaxis]\n",
    ")(ids_ent_cod)\n",
    "\n",
    "Z = embeddings_cod\n",
    "for _ in range(N):\n",
    "    salto = Z\n",
    "    capa_atencion = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_cabezas, key_dim=tam_emb, dropout=tasa_dropout)\n",
    "    Z = capa_atencion(Z, value=Z, attention_mask=masc_cod)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, salto]))\n",
    "    salto = Z\n",
    "    Z = tf.keras.layers.Dense(n_unidades, activation=\"relu\")(Z)\n",
    "    Z = tf.keras.layers.Dense(tam_emb)(Z)\n",
    "    Z = tf.keras.layers.Dropout(tasa_dropout)(Z)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, salto]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "H1byr1KOP9nH"
   },
   "outputs": [],
   "source": [
    "#decoder_pad_mask = tf.math.not_equal(decoder_input_ids, 0)[:, tf.newaxis]\n",
    "masc_dec = tf.keras.layers.Lambda(\n",
    "    lambda x: tf.math.not_equal(x, 0)[:, tf.newaxis]\n",
    ")(ids_ent_dec)\n",
    "\n",
    "causal_mask = tf.linalg.band_part(  # Matriz triangular inferior\n",
    "    tf.ones((max_long_lote_dec, max_long_lote_cod), tf.bool), -1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "X4M6oXSxQDDn"
   },
   "outputs": [],
   "source": [
    "# Decodificador\n",
    "salida_cod = Z  # Se almacena la salida final de codificador\n",
    "Z = embeddings_dec  # El decodificador inicar con su entrada propia\n",
    "for _ in range(N):\n",
    "    salto = Z\n",
    "    capa_atencion = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_cabezas, key_dim=tam_emb, dropout=tasa_dropout)\n",
    "    Z = capa_atencion(Z, value=Z, attention_mask=causal_mask & masc_dec)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, salto]))\n",
    "    salto = Z\n",
    "    capa_atencion = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_cabezas, key_dim=tam_emb, dropout=tasa_dropout)\n",
    "    Z = capa_atencion(Z, value=salida_cod, attention_mask=masc_cod)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, salto]))\n",
    "    salto = Z\n",
    "    Z = tf.keras.layers.Dense(n_unidades, activation=\"relu\")(Z)\n",
    "    Z = tf.keras.layers.Dense(tam_emb)(Z)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, salto]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "eqyQbzfIQIjC"
   },
   "outputs": [],
   "source": [
    "Y_proba = tf.keras.layers.Dense(tam_voc, activation=\"softmax\")(Z)\n",
    "modelo = tf.keras.Model(inputs=[ent_cod, ent_dec],\n",
    "                       outputs=[Y_proba])\n",
    "modelo.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0LqY0PAZQKj_",
    "outputId": "09abcb29-7b68-45fd-cebe-66c5488787b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 69ms/step - accuracy: 0.9402 - loss: 0.3696 - val_accuracy: 0.9641 - val_loss: 0.1562\n",
      "Epoch 2/10\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 69ms/step - accuracy: 0.9646 - loss: 0.1531 - val_accuracy: 0.9681 - val_loss: 0.1339\n",
      "Epoch 3/10\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 69ms/step - accuracy: 0.9678 - loss: 0.1344 - val_accuracy: 0.9695 - val_loss: 0.1269\n",
      "Epoch 4/10\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 70ms/step - accuracy: 0.9694 - loss: 0.1250 - val_accuracy: 0.9707 - val_loss: 0.1212\n",
      "Epoch 5/10\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 69ms/step - accuracy: 0.9706 - loss: 0.1186 - val_accuracy: 0.9711 - val_loss: 0.1176\n",
      "Epoch 6/10\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 71ms/step - accuracy: 0.9715 - loss: 0.1136 - val_accuracy: 0.9720 - val_loss: 0.1138\n",
      "Epoch 7/10\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 69ms/step - accuracy: 0.9721 - loss: 0.1101 - val_accuracy: 0.9725 - val_loss: 0.1121\n",
      "Epoch 8/10\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 69ms/step - accuracy: 0.9727 - loss: 0.1067 - val_accuracy: 0.9728 - val_loss: 0.1109\n",
      "Epoch 9/10\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 69ms/step - accuracy: 0.9733 - loss: 0.1040 - val_accuracy: 0.9731 - val_loss: 0.1095\n",
      "Epoch 10/10\n",
      "\u001b[1m3750/3750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 69ms/step - accuracy: 0.9737 - loss: 0.1021 - val_accuracy: 0.9733 - val_loss: 0.1089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7e5fddf6dae0>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
    "          validation_data=((X_valid, X_valid_dec), Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 192
    },
    "id": "AeMR_vdVjrMg",
    "outputId": "ee8c4a6b-a33b-4bf8-ed54-2f44b70942c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'me gusta el fútbol y estudiar en casa'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traducir(\"I like soccer and studying at home\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K7Uzsq2o4Nxj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OmtjQv4dKphL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
